#!/usr/bin/env node

/**
 * Setup script for local AI development
 * This script helps configure local AI models for development without API keys
 */

import { execSync } from 'child_process';
import { readFileSync, writeFileSync, existsSync } from 'fs';
import { join } from 'path';

const ENV_FILE = '.env.local';

console.log('ü§ñ Setting up local AI development environment...\n');

async function checkOllama() {
  try {
    const response = await fetch('http://localhost:11434/api/tags');
    if (response.ok) {
      const data = await response.json();
      const models = data.models?.map(m => m.name) || [];
      console.log('‚úÖ Ollama is running');
      console.log(`üì¶ Available models: ${models.join(', ') || 'none'}`);
      return models;
    }
  } catch (error) {
    console.log('‚ùå Ollama not running on localhost:11434');
    return [];
  }
}

function updateEnvFile(config) {
  let envContent = '';
  
  if (existsSync(ENV_FILE)) {
    envContent = readFileSync(ENV_FILE, 'utf8');
  }

  // Add or update configuration
  const lines = envContent.split('\n');
  const newLines = [];
  const configKeys = Object.keys(config);
  const existingKeys = new Set();

  // Update existing lines
  for (const line of lines) {
    const [key] = line.split('=');
    if (configKeys.includes(key)) {
      newLines.push(`${key}=${config[key]}`);
      existingKeys.add(key);
    } else {
      newLines.push(line);
    }
  }

  // Add new configuration
  for (const [key, value] of Object.entries(config)) {
    if (!existingKeys.has(key)) {
      newLines.push(`${key}=${value}`);
    }
  }

  writeFileSync(ENV_FILE, newLines.join('\n'));
}

async function main() {
  const models = await checkOllama();
  
  console.log('\nüîß Configuration options:\n');
  console.log('1. Mock Model (fastest, no setup required)');
  console.log('2. Ollama (real AI, requires installation)');
  console.log('3. Keep existing configuration\n');

  // For automation, we'll set up mock by default
  const choice = process.argv[2] || '1';

  switch (choice) {
    case '1': {
      console.log('üé≠ Configuring mock model...');
      updateEnvFile({
        'VITE_DEV_MODEL_PROVIDER': 'mock',
        'VITE_FORCE_LOCAL_MODEL': 'true',
        'VITE_DISABLE_AUTH': '1'
      });
      console.log('‚úÖ Mock model configured! No additional setup required.');
      break;
    }

    case '2': {
      if (models.length === 0) {
        console.log('‚ö†Ô∏è  Ollama not detected. Installing...');
        console.log('üìñ Visit https://ollama.ai for installation instructions');
        console.log('üí° Quick setup: curl -fsSL https://ollama.ai/install.sh | sh');
        console.log('üöÄ Then run: ollama serve && ollama pull llama3.2');
      }

      updateEnvFile({
        'VITE_DEV_MODEL_PROVIDER': 'ollama',
        'VITE_FORCE_LOCAL_MODEL': 'true',
        'OLLAMA_MODEL': models[0] || 'llama3.2',
        'VITE_DISABLE_AUTH': '1'
      });
      console.log('‚úÖ Ollama configuration updated!');
      break;
    }

    case '3': {
      console.log('‚è≠Ô∏è  Keeping existing configuration');
      break;
    }

    default: {
      console.log('‚ùì Invalid choice, using mock model as default');
      updateEnvFile({
        'VITE_DEV_MODEL_PROVIDER': 'mock',
        'VITE_FORCE_LOCAL_MODEL': 'true'
      });
    }
  }

  console.log('\nüéâ Setup complete!');
  console.log('\nüí° Tips:');
  console.log('‚Ä¢ Run `pnpm dev` to start development server');
  console.log('‚Ä¢ Authentication is disabled for local development');
  console.log('‚Ä¢ Switch models by changing VITE_DEV_MODEL_PROVIDER in .env.local');
  console.log('\nüìä Model comparison:');
  console.log('Mock:   ‚ö° Instant ‚Ä¢ üì¶ No setup ‚Ä¢ üé≠ Fake responses');
  console.log('Ollama: ü§ñ Real AI ‚Ä¢ üíæ Local ‚Ä¢ üîí Private');
}

main().catch(console.error);
